from typing import Dict, List, Optional, Any
import pandas as pd
import numpy as np
from rapidfuzz import fuzz
from utils.normalize import norm_col, norm_key, values_equal


def _build_composite_key(df: pd.DataFrame, key_cols: List[str]) -> pd.Series:
    """Build composite key from multiple columns."""
    # Normalize each key column and concatenate with |
    key_parts = [df[col].apply(norm_key) for col in key_cols]
    return key_parts[0].str.cat(key_parts[1:], sep='|')


def _read_financial_file(path: str) -> pd.DataFrame:
    """Read financial Excel file and normalize column names with inline header detection."""
    from utils.constants import FINANCIAL_EXPECTED_COLUMNS
    
    # Read entire file with no header (single read instead of 2 reads)
    df_raw = pd.read_excel(path, header=None, nrows=50, engine='calamine')
    
    # Find header row in-memory (previously done via separate detect_header_row call)
    expected_norm = set([norm_col(col) for col in FINANCIAL_EXPECTED_COLUMNS if norm_col(col)])
    
    best_row = 0
    best_score = -1
    
    for r in range(min(10, len(df_raw))):  # Search first 10 rows
        values = [norm_col(str(val)) for val in df_raw.iloc[r].values if pd.notna(val)]
        score = sum(1 for v in values if v in expected_norm)
        
        if score > best_score:
            best_score = score
            best_row = r
    
    if best_score == 0:
        raise ValueError("No header row detected with expected financial columns")
    
    # Now read the full file with the detected header row (single full read)
    df = pd.read_excel(path, header=best_row, engine='calamine')
    
    # Remove unnamed columns
    df = df.loc[:, ~df.columns.to_series().apply(lambda x: str(x).startswith('Unnamed'))]
    
    # Normalize column names
    df.columns = [norm_col(col) for col in df.columns]
    
    return df


def read_financial_files_parallel(current_path: str, previous_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Read both financial files in parallel for faster processing."""
    from concurrent.futures import ThreadPoolExecutor
    
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_current = executor.submit(_read_financial_file, current_path)
        future_previous = executor.submit(_read_financial_file, previous_path)
        
        df_current = future_current.result()
        df_previous = future_previous.result()
    
    return df_current, df_previous


def validate_financial_files(
    current_path: str,
    previous_path: str,
    key_cols: List[str]
) -> Dict[str, Any]:
    """
    Validate financial files and return statistics about duplicates plus prepared DataFrames.
    
    Args:
        current_path: Path to current file
        previous_path: Path to previous file
        key_cols: List of key column names
    
    Returns:
        Dictionary with validation statistics and prepared DataFrames
    """
    # Read files in parallel (much faster than sequential)
    df_current, df_previous = read_financial_files_parallel(current_path, previous_path)
    
    # Build composite keys
    df_current['_composite_key'] = _build_composite_key(df_current, key_cols)
    df_previous['_composite_key'] = _build_composite_key(df_previous, key_cols)
    
    # Check for duplicates
    current_total = len(df_current)
    previous_total = len(df_previous)
    
    current_duplicates = []
    current_dup_count = 0
    if df_current['_composite_key'].duplicated().any():
        dup_mask = df_current['_composite_key'].duplicated(keep='first')
        current_dup_count = dup_mask.sum()
        dup_keys = df_current[dup_mask]['_composite_key'].unique()
        current_duplicates = dup_keys[:5].tolist()  # Sample of 5
    
    previous_duplicates = []
    previous_dup_count = 0
    if df_previous['_composite_key'].duplicated().any():
        dup_mask = df_previous['_composite_key'].duplicated(keep='first')
        previous_dup_count = dup_mask.sum()
        dup_keys = df_previous[dup_mask]['_composite_key'].unique()
        previous_duplicates = dup_keys[:5].tolist()  # Sample of 5
    
    # De-duplicate the DataFrames
    if current_dup_count > 0:
        df_current = df_current.drop_duplicates(subset='_composite_key', keep='first')
    if previous_dup_count > 0:
        df_previous = df_previous.drop_duplicates(subset='_composite_key', keep='first')
    
    current_unique = current_total - current_dup_count
    previous_unique = previous_total - previous_dup_count
    
    return {
        "current_total_rows": current_total,
        "current_unique_keys": current_unique,
        "current_duplicate_rows": current_dup_count,
        "current_duplicate_samples": current_duplicates,
        "previous_total_rows": previous_total,
        "previous_unique_keys": previous_unique,
        "previous_duplicate_rows": previous_dup_count,
        "previous_duplicate_samples": previous_duplicates,
        "has_duplicates": current_dup_count > 0 or previous_dup_count > 0,
        # Return prepared DataFrames for caching
        "df_current": df_current,
        "df_previous": df_previous
    }


def run_mastergroup_validation(df: pd.DataFrame, threshold: int = 70) -> pd.DataFrame:
    """
    Analysis 1: Mastergroup Mapping Validation
    
    Uses fuzzy matching to validate if Site Customer Name logically matches Mastergroup Name.
    Returns the dataframe with added 'Match Status' and 'Fuzzy Score' columns.
    
    Args:
        df: DataFrame with 'Mastergroup Name' and 'Site Customer Name' columns
        threshold: Minimum fuzzy score for a match (default: 70)
    
    Returns:
        DataFrame with 'Match Status' (Matched/Not Matched) and 'Fuzzy Score' columns
    """
    result_df = df.copy()
    
    # Vectorized approach: extract columns as lists and compute scores in batch
    mg_vals = result_df['Mastergroup Name'].fillna('').astype(str).str.strip().tolist()
    sc_vals = result_df['Site Customer Name'].fillna('').astype(str).str.strip().tolist()
    
    # Compute fuzzy scores using rapidfuzz (much faster than thefuzz)
    scores = [fuzz.token_set_ratio(a, b) if a and b else 0 for a, b in zip(mg_vals, sc_vals)]
    
    result_df['Fuzzy Score'] = scores
    result_df['Match Status'] = ['Matched' if s >= threshold else 'Not Matched' for s in scores]
    
    return result_df


def run_non_financial_comparison(
    df_current: pd.DataFrame,
    df_previous: pd.DataFrame,
    key_cols: List[str],
    compare_cols: List[str]
) -> pd.DataFrame:
    """
    Analysis 2: Non-Financial Field Comparison
    
    Compares non-financial fields between current and previous files.
    Identifies Added, Deleted, and Modified records.
    
    Args:
        df_current: Current period DataFrame
        df_previous: Previous period DataFrame
        key_cols: List of key column names
        compare_cols: List of columns to compare
    
    Returns:
        DataFrame with Change Type and detail columns
    """
    # Build composite keys if not already present
    df_current = df_current.copy()
    df_previous = df_previous.copy()
    
    if '_composite_key' not in df_current.columns:
        df_current['_composite_key'] = _build_composite_key(df_current, key_cols)
    if '_composite_key' not in df_previous.columns:
        df_previous['_composite_key'] = _build_composite_key(df_previous, key_cols)
    
    # Check for empty keys
    if (df_current['_composite_key'] == '').any():
        raise ValueError("Empty composite keys found in current file")
    if (df_previous['_composite_key'] == '').any():
        raise ValueError("Empty composite keys found in previous file")
    
    # Handle duplicates: keep first occurrence
    if df_current['_composite_key'].duplicated().any():
        df_current = df_current.drop_duplicates(subset='_composite_key', keep='first')
    if df_previous['_composite_key'].duplicated().any():
        df_previous = df_previous.drop_duplicates(subset='_composite_key', keep='first')
    
    # Vectorized approach: use merge to identify added/deleted/common
    merged = df_current.merge(
        df_previous, 
        on='_composite_key', 
        how='outer', 
        suffixes=('_curr', '_prev'),
        indicator=True
    )
    
    changes = []
    
    # Added records (only in current)
    added = merged[merged['_merge'] == 'left_only']
    for _, row in added.iterrows():
        changes.append({
            'Change Type': 'Added',
            'Composite Key': row['_composite_key'],
            **{col: row.get(f'{col}_curr', '') for col in key_cols},
            'Changed Fields': 'All (new record)',
            'Details': 'Record added in current file'
        })
    
    # Deleted records (only in previous)
    deleted = merged[merged['_merge'] == 'right_only']
    for _, row in deleted.iterrows():
        changes.append({
            'Change Type': 'Deleted',
            'Composite Key': row['_composite_key'],
            **{col: row.get(f'{col}_prev', '') for col in key_cols},
            'Changed Fields': 'All (deleted record)',
            'Details': 'Record removed from current file'
        })
    
    # Modified records (in both) - vectorized comparison
    common = merged[merged['_merge'] == 'both']
    for _, row in common.iterrows():
        changed_fields = []
        field_details = []
        
        for col in compare_cols:
            if col in key_cols:
                continue
            
            val_curr = row.get(f'{col}_curr', None)
            val_prev = row.get(f'{col}_prev', None)
            
            if not values_equal(val_curr, val_prev):
                changed_fields.append(col)
                old_val = val_prev if pd.notna(val_prev) else ''
                new_val = val_curr if pd.notna(val_curr) else ''
                field_details.append(f"{col}: '{old_val}' â†’ '{new_val}'")
        
        if changed_fields:
            changes.append({
                'Change Type': 'Modified',
                'Composite Key': row['_composite_key'],
                **{col: row.get(f'{col}_curr', '') for col in key_cols},
                'Changed Fields': ', '.join(changed_fields),
                'Details': ' | '.join(field_details)
            })
    
    return pd.DataFrame(changes)


def run_financial_comparison(
    df_current: pd.DataFrame,
    df_previous: pd.DataFrame,
    key_cols: List[str]
) -> pd.DataFrame:
    """
    Analysis 3: Financial Comparison
    
    Compares Total Operating Income between current and previous files.
    
    Args:
        df_current: Current period DataFrame
        df_previous: Previous period DataFrame
        key_cols: List of key column names
    
    Returns:
        DataFrame with TOI Status column
    """
    df_current = df_current.copy()
    df_previous = df_previous.copy()
    
    # Build composite keys if not already present
    if '_composite_key' not in df_current.columns:
        df_current['_composite_key'] = _build_composite_key(df_current, key_cols)
    if '_composite_key' not in df_previous.columns:
        df_previous['_composite_key'] = _build_composite_key(df_previous, key_cols)
    
    # Handle duplicates: keep first occurrence
    if df_current['_composite_key'].duplicated().any():
        df_current = df_current.drop_duplicates(subset='_composite_key', keep='first')
    if df_previous['_composite_key'].duplicated().any():
        df_previous = df_previous.drop_duplicates(subset='_composite_key', keep='first')
    
    toi_col = 'Total Operating Income (HORIS YTD Financials)'
    
    # Prepare dataframes with only needed columns
    curr_cols = ['_composite_key'] + key_cols + [toi_col]
    prev_cols = ['_composite_key', toi_col]
    
    df_curr_subset = df_current[curr_cols].copy()
    df_prev_subset = df_previous[prev_cols].copy()
    
    # Vectorized merge: full outer join
    merged = df_curr_subset.merge(
        df_prev_subset,
        on='_composite_key',
        how='outer',
        suffixes=('_curr', '_prev')
    )
    
    # Convert TOI columns to numeric
    merged['TOI Current'] = pd.to_numeric(merged.get(f'{toi_col}_curr', merged.get(toi_col, 0)), errors='coerce').fillna(0)
    merged['TOI Previous'] = pd.to_numeric(merged.get(f'{toi_col}_prev', 0), errors='coerce').fillna(0)
    merged['TOI Difference'] = merged['TOI Current'] - merged['TOI Previous']
    
    # Vectorized status assignment using np.select
    conditions = [
        (merged['TOI Current'] == 0) & (merged['TOI Previous'] == 0),
        merged['TOI Current'] > merged['TOI Previous'],
        merged['TOI Current'] < merged['TOI Previous'],
        merged['TOI Current'] == merged['TOI Previous'],
        merged[f'{toi_col}_curr'].isna(),  # Only in previous
        merged[f'{toi_col}_prev'].isna()   # Only in current
    ]
    
    choices = [
        'No TOI data in current and previous data for this Site Customer',
        'Increased in current file',
        'Decreased in current file',
        'No TOI change in previous file and current file',
        'No details of Site Customer in current file',
        'No details of Site Customer in previous file'
    ]
    
    merged['TOI Status'] = np.select(conditions, choices, default='No TOI change in previous file and current file')
    
    # Build result with key columns
    result_cols = ['_composite_key'] + key_cols + ['TOI Previous', 'TOI Current', 'TOI Difference', 'TOI Status']
    
    # For key columns, prefer current values, fall back to previous
    for col in key_cols:
        if f'{col}_curr' in merged.columns:
            merged[col] = merged[f'{col}_curr'].fillna(merged.get(f'{col}_prev', ''))
        elif col not in merged.columns:
            merged[col] = ''
    
    # Rename composite key column
    merged = merged.rename(columns={'_composite_key': 'Composite Key'})
    result_cols[0] = 'Composite Key'
    
    return merged[result_cols]


def build_consolidated(
    df_mastergroup: pd.DataFrame,
    df_nonfinancial: pd.DataFrame,
    df_financial: pd.DataFrame,
    key_cols: List[str]
) -> pd.DataFrame:
    """
    Build consolidated report merging all three analyses.
    
    Args:
        df_mastergroup: Result from run_mastergroup_validation
        df_nonfinancial: Result from run_non_financial_comparison
        df_financial: Result from run_financial_comparison
        key_cols: List of key column names
    
    Returns:
        Consolidated DataFrame with all statuses
    """
    # Build composite keys for all dataframes
    df_mg = df_mastergroup.copy()
    df_mg['_composite_key'] = _build_composite_key(df_mg, key_cols)
    
    # Handle duplicates in mastergroup: keep first occurrence
    if df_mg['_composite_key'].duplicated().any():
        dup_mask = df_mg['_composite_key'].duplicated(keep='first')
        df_mg = df_mg[~dup_mask].copy()
    
    # For non-financial, composite key already exists
    df_nf = df_nonfinancial.copy()
    if 'Composite Key' in df_nf.columns:
        df_nf = df_nf.rename(columns={'Composite Key': '_composite_key'})
    
    # For financial, composite key already exists
    df_fin = df_financial.copy()
    if 'Composite Key' in df_fin.columns:
        df_fin = df_fin.rename(columns={'Composite Key': '_composite_key'})
    
    # Start with mastergroup validation (has all current records)
    consolidated = df_mg.set_index('_composite_key')
    
    # Merge non-financial changes - include Details column
    nf_cols = ['Change Type', 'Details'] if 'Details' in df_nf.columns else ['Change Type']
    nf_status = df_nf.set_index('_composite_key')[nf_cols] if not df_nf.empty else pd.DataFrame()
    if not nf_status.empty:
        consolidated = consolidated.join(nf_status, how='left')
    else:
        consolidated['Change Type'] = 'No Change'
        consolidated['Details'] = ''
    
    # Fill NaN in Change Type with 'No Change'
    consolidated['Change Type'] = consolidated['Change Type'].fillna('No Change')
    if 'Details' in consolidated.columns:
        consolidated['Details'] = consolidated['Details'].fillna('')
    
    # Merge financial comparison
    fin_cols = ['TOI Status', 'TOI Previous', 'TOI Current', 'TOI Difference']
    fin_status = df_fin.set_index('_composite_key')[fin_cols] if not df_fin.empty else pd.DataFrame()
    if not fin_status.empty:
        consolidated = consolidated.join(fin_status, how='left')
    else:
        for col in fin_cols:
            consolidated[col] = 'N/A' if col == 'TOI Status' else 0
    
    # Fill NaN in TOI Status
    consolidated['TOI Status'] = consolidated['TOI Status'].fillna('N/A')
    
    # Reset index to get composite key as column
    consolidated = consolidated.reset_index()
    
    # Reorder columns for better readability - include Details
    key_columns = key_cols + ['Match Status', 'Fuzzy Score', 'Change Type', 'Details', 'TOI Status', 
                               'TOI Previous', 'TOI Current', 'TOI Difference']
    other_columns = [col for col in consolidated.columns if col not in key_columns and col != '_composite_key']
    
    final_columns = key_columns + other_columns
    final_columns = [col for col in final_columns if col in consolidated.columns]
    
    return consolidated[final_columns]
